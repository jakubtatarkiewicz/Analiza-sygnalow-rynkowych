---
title: "Analiza sygnałów rynkowych"
author: "Jakub Tatarkiewicz"
editor: visual
lang: pl
format:
  html:
    toc: true
    toc-location: left
    math: true
    toc-depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

# Sygnały rynkowe

Sygnały rynkowe w ujęciu rynków finansowych, to zdefiniowane w pewien określony sposób momenty w czasie, które dają informacje o stanie rynku lub pewnej jego części. Należy pamiętać, że jest to pojęcie potoczne i każdy może je definiować dla swoich potrzeb inaczej. Tym nie mniej na potrzeby ich analizy, właśnie w ten sposób zostają określone. Informacja pochodząca z sygnału, ma pomóc inwestorom w ocenie kierunku dążenia instumentów finansowych i co za tym idzie w zwiększeniu stopy zwrotu jego portfela. Dobór sygnałów jest kluczowy dla sukcesu gracza rynkowego. Kluczem jest znalezienie takich, które są stale efektywne w predykcji. Właśnie pod tym kątem będą analizowane sygnały tj. jaka jest ich efektywność na względnie dużym przedziale czasowym i w pewnym środowisku. Sukcesem będzie też dowiedzenie, że sygnał nie jest rentowny pomimo wcześniejszego przekonania o jego działaniu. Unikanie strat jest ważniejsze niż zdobywanie zysków.

## Zbiór danych

Niezbędny będzie zbiór danych zawierający historyczne stany rynków (akcji i obligacji). Nie zawsze cena będzie przedmiotem uwagi. Wszelkie potrzebne dane znajdują się na następujących stronach:

-   [FRED - Federal Reserve Economic Data](https://fred.stlouisfed.org/)
-   [Yahoo Finance](https://finance.yahoo.com/)

## Pobieranie danych

Z wyżej wymienionych stron pochodzą następujące dane:

### FRED:

Dane dzienne **rentowności obligacji 10-letnich oraz 2-letnich**.

```{r}
#| eval: false
#| echo: false
fetch_fred_data <- function(symbol, name = NULL) {
  if (is.null(name)) {
    name <- symbol
  }
  
  url <- paste0("https://fred.stlouisfed.org/graph/fredgraph.csv?id=", 
                symbol, 
                "&cosd=1800-01-01&coed=2100-01-01")
  
  dir.create("data/fred", recursive = TRUE, showWarnings = FALSE)
  filename <- file.path("data/fred", paste0(name, ".csv"))
  
  tryCatch({
    download.file(url, filename, mode = "wb")
  }, error = function(e) {
    cat(paste("Failed to download", name, "(", symbol, "):", e$message, "\n"))
  })
}

fetch_fred_data('GDP') # Produkt krajowy brutto
fetch_fred_data('UNRATE') # Stopa bezrobocia
fetch_fred_data('FEDFUNDS') # Stopa procentowa
fetch_fred_data('CPIAUCSL', 'CPI') # Wskaźnik cen konsumenckich
fetch_fred_data('DGS10', '10BY') # Rentowność obliacji 10 letnich
fetch_fred_data('DGS2', '2BY') # Rentowność obligacji 2 letnich
fetch_fred_data('DGS3MO', '3MOBY') # Rentowność obligacji 3 miesięcznych
```

### Yahoo Finance

Indeks **SP&500** czyli 500 największych spółek z giełd USA. Są to dane dziennych świeczek zawierających:

-   Cene otwarcia
-   Cenę zamknięcia
-   Cenę minimialną
-   Cenę maksymalną
-   Wolumen transakcji

```{r}
#| eval: false
#| echo: false
suppressPackageStartupMessages(
  library(quantmod)
)

fetch_yahoo_data <- function(symbol, 
                             name = NULL, 
                             from = "1900-01-01", 
                             to = Sys.Date()) {
  
  if (is.null(name)) {
    name <- symbol
  }
  
  dir.create("data/yahoo", recursive = TRUE, showWarnings = FALSE)
  filename <- file.path("data/yahoo", paste0(name, ".csv"))
  
  tryCatch({
    suppressWarnings(
      suppressMessages(
        data <- getSymbols(
          symbol,
          src = "yahoo",
          from = from,
          to   = to,
          auto.assign = FALSE
        )
      )
    )
    write.zoo(data, file = filename, sep = ",")
  }, error = function(e) {
    cat(paste("Failed to download", name, "(", symbol, "):", e$message, "\n"))
  })
}

fetch_yahoo_data("DX-Y.NYB", "DXY") # Indeks dolara
#fetch_yahoo_data("GC=F", "GOLD") # Kontrakt na złoto
#fetch_yahoo_data("CL=F", "OIL") # Kontrakt na rope
fetch_yahoo_data("^GSPC", "US500") # Indeks SP&500
#fetch_yahoo_data("BTC-USD", "BTC") # Bitcoin
```

```{r}
#| eval: false
#| echo: false
library(netstat)
library(tidyverse)
library(RSelenium)
library(binman)
library(dplyr)

closeSignupDialog <- function(driver) {
  tryCatch({
    close_button <- driver$findElement(using = "css selector", value = "svg[data-test='sign-up-dialog-close-button']")
    close_button$clickElement()
  }, error = function(e){
    message("Caught exception: ", e$message)
  })
}

closeAcceptDialog <- function(driver) {
  tryCatch({
    button <- driver$findElement(using = "id", value = "onetrust-accept-btn-handler")
    button$mouseMoveToLocation(x = 1, y = 2)
    button$clickElement()
  }, error = function(e){
    message("Caught exception: ", e$message)
  })
}

changeDate <- function(driver, date) {
  closeSignupDialog(driver)
  calendar_button <- driver$findElement(using = "css selector", value = "div.flex.flex-1.items-center.gap-3\\.5.rounded.border")
      calendar_button$clickElement()
  
  Sys.sleep(0.5)
  closeSignupDialog()
  data_field <- driver$findElement(using = "css selector", ".absolute.left-0.top-0.h-full.w-full.opacity-0")
  data_field$clickElement()
  
  Sys.sleep(0.5)
  
  closeSignupDialog()
  driver$executeScript(
    paste0(
      "document.querySelector('input[type=\"date\"].absolute.left-0.top-0.h-full.w-full.opacity-0').value = '",
      date,
      "'"
    )
  )
  data_field$clickElement()
  Sys.sleep(0.5)
  
  data_field$sendKeysToActiveElement(list("\uE012")) # Left arrow
  data_field$sendKeysToActiveElement(list("\uE014")) # Right arrow
  Sys.sleep(0.5)
  
  button <- driver$findElement(
      using = "css selector", 
      "div.flex.cursor-pointer.items-center.gap-3.rounded.bg-v2-blue"
  )
  button$clickElement()
}

convertVolume0 <- function(volume) {
  if (grepl("K", volume)) {
    as.numeric(sub("K", "", volume)) * 1e3
  } else if (grepl("M", volume)) {
    as.numeric(sub("M", "", volume)) * 1e6
  } else {
    as.numeric(volume)
  }
}

convertVolume <- function(volume) {
  as.integer(convertVolume0(volume))
}

latestDataRow <- function(driver) {
  row <- driver$findElement(using = "css selector", "tr.historical-data-v2_price__atUfP")
  
  return(as.Date(
      trimws(row$findChildElements(using = "css selector", "td")[[1]]$getElementText()[[1]]),
      format = "%b %d, %Y"))
}

toNumeric <- function(text) {
  return(as.numeric(gsub(",", "", text)))
}

initialDataFrame <- function(symbol) {
  file_path <- paste0("data/investing/", symbol, "_BASE.csv")
  if (file.exists(file_path)) {
    return(read.csv(file_path, stringsAsFactors = FALSE, colClasses = c(date = "Date")))
  } else {
    return(data.frame(
      date = as.Date(character()),
      close = numeric(),
      open = numeric(),
      high = numeric(),
      low = numeric(),
      volume = numeric()
    ))
  }
}

readDataFromPage <- function(driver, data) {
  Sys.setlocale("LC_TIME", "C")
  rows <- driver$findElements(using = "css selector", "tr.historical-data-v2_price__atUfP")
  
  data_list <- list()
  
  for (i in seq_along(rows)) {
    tryCatch({
      td <- rows[[i]]$findChildElements(using = "css selector", "td")

      date <- as.Date(trimws(td[[1]]$getElementText()[[1]]), format = "%b %d, %Y")
      close <- toNumeric(td[[2]]$getElementText()[[1]])
      open <- toNumeric(td[[3]]$getElementText()[[1]])
      high <- toNumeric(td[[4]]$getElementText()[[1]])
      low <- toNumeric(td[[5]]$getElementText()[[1]])
      volume <- convertVolume(td[[6]]$getElementText()[[1]])
      
      # Append cleaned data to the list
      data_list[[i]] <- list(
        date = date,
        close = close,
        open = open,
        high = high,
        low = low,
        volume = volume
      )
    }, error = function(e) {
      message("Error processing row ", i, ": ", e$message)
    })
  }
  

  extracted_data <- do.call(rbind, lapply(data_list, as.data.frame))
  
  if (nrow(extracted_data) > 0) {
    extracted_data <- extracted_data[nrow(extracted_data):1, ]
  }
  combined_data <- dplyr::bind_rows(data, extracted_data) %>%
    dplyr::distinct(date, .keep_all = TRUE)
  
  print(paste0("Combined data max date ", max(combined_data$date)))
  
  return(combined_data)
}

readAndSaveData <- function(driver, symbol, link) {
  df <- initialDataFrame(symbol)

  df_latest_date <- if (nrow(df) == 0) {
    df_latest_date <- as.Date("1900-01-01")
  } else {
    df_latest_date <- max(df$date, na.rm = TRUE)
  }
  
  driver$navigate(link)
  
  Sys.sleep(2)
  closeAcceptDialog(driver)
  Sys.sleep(0.5)
  closeSignupDialog(driver)
  
  latest_date <- latestDataRow(driver)
  if(is.na(latest_date)) { print("latest_date is na") } else print(paste("latest date: ", latest_date))
  print(paste("df latest date: ", df_latest_date))

  repeat {
    Sys.sleep(3)
    changeDate(driver, df_latest_date)
    Sys.sleep(10)
    df <- readDataFromPage(driver, df)
    df_latest_date <- max(df$date)
    print(latest_date)
    print(df_latest_date)
    if(!is.na(df_latest_date) && !is.na(latest_date) && df_latest_date == latest_date) {
      break;
    }
  }

  write.csv(df, paste0("data/investing/", symbol, ".csv"), row.names = FALSE)
}

rs_driver_obj <- rsDriver(
  browser = 'chrome',
  chromever = '131.0.6778.85',
  port = 4444L,
  version = "2.53.1",
  extraCapabilities = list(
    chromeOptions = list(
      args = list("--log-level=DEBUG")#, "--headless", "--disable-gpu", "--no-sandbox")
    )
  )
)

driver <- rs_driver_obj$client

readAndSaveData(driver, "OIL", "https://www.investing.com/commodities/crude-oil-historical-data")
readAndSaveData(driver, "GOLD", "https://www.investing.com/commodities/gold-historical-data")

rs_driver_obj$server$stop()
```

## Analiza sygnałów

### 10-2Y

#### Definicja i wystąpienia

Sygnał nazywany 10Y-2Y jest definiowany jako momenty trwałego przejścia wartości różnicy rentowności obligacji 10 letnich i 2 letnich z wartości ujemnych na dodatnie. Analogicznym sygnałem jest 10Y-3M z 3 miesięcznymi obligacjami.

```{r}
#| echo: false
library(tidyverse)


d10 <- read_csv(
  "data/fred/DGS10.csv", 
  col_types = cols(
    DATE  = col_date(format = "%Y-%m-%d"), 
    DGS10 = col_double()
  )
)

d2 <- read_csv(
  "data/fred/DGS2.csv", 
  col_types = cols(
    DATE = col_date(format = "%Y-%m-%d"), 
    DGS2 = col_double()
  )
)


df <- full_join(d10, d2, by = "DATE") %>%
  arrange(DATE) %>%
  filter(!is.na(DGS10), !is.na(DGS2)) %>%
  mutate(
    yield_curve = DGS10 - DGS2,
    sign = yield_curve > 0
  )

df <- df %>%
  mutate(
    crossing_neg2pos = sign == TRUE & lag(sign) == FALSE,
    crossing_neg2pos = replace_na(crossing_neg2pos, FALSE)
  )

df <- df %>%
  mutate(idx = row_number())

get_mean_next_90 <- function(i) {
  end_i <- i + 89
  if (end_i > nrow(df)) {
    return(NA_real_)
  } else {
    return(mean(df$yield_curve[i:end_i], na.rm = TRUE))
  }
}

df <- df %>%
  rowwise() %>%
  mutate(
    mean_90 = if_else(
      crossing_neg2pos,
      get_mean_next_90(idx),
      NA_real_
    )
  ) %>%
  ungroup() %>%
  mutate(
    is_permanent = crossing_neg2pos & mean_90 > 0
  )


permanent_signals <- df %>%
  filter(is_permanent) %>%
  select(idx, DATE)

final_signal_idx <- c()
last_signal_date <- as.Date("1900-01-01")

for (i in seq_len(nrow(permanent_signals))) {
  this_date <- permanent_signals$DATE[i]
  
  if (this_date - last_signal_date >= 365) {
    final_signal_idx <- c(final_signal_idx, permanent_signals$idx[i])
    last_signal_date <- this_date
  }
}

df <- df %>%
  mutate(is_signal = idx %in% final_signal_idx)


ggplot(df, aes(x = DATE, y = yield_curve)) +
  geom_line(color = "steelblue") +
  geom_hline(yintercept = 0, color = "red") +
  
  geom_vline(
    data = filter(df, is_signal),
    aes(xintercept = DATE),
    color = "blue", 
    linetype = "dashed", 
    size = 0.8
  ) +
  
  labs(
    title = "Przejścia krzywej rentowności (10Y - 2Y)",
    subtitle = "przejście z ujemnych na dodatnie i kolejne 60 dni z dodatnią średnią",
    x = "Data",
    y = "Różnica rentowności (10Y - 2Y)"
  ) +
  theme_minimal()


```

I dodając do tego wykres indeksu giełdowego SP&500 w postaci:

$$
\frac{\log(\text{Wartość SP\&500})}{\text{Wartość predykcji regresji liniowej } \log(\text{Wartość SP\&500})}
$$

#### Przekształcenie indeksu w oscylator

Logarytmując indeks, widać zrównoważenie jego wzrostu, a korzystająć z tego że przypomina on oscycylację wobec prostej, wypłaszczyć go, dzieląc przez współczynnik regresji liniowej stanowiący tą prostą. Następnie MinMaxScaler służy do wyskalowania.

```{r}
#| echo: false
library(tidyverse)
sp500 <- read_csv(
  "data/yahoo/US500.csv",
  col_types = cols(
    Index         = col_date(format = "%Y-%m-%d"),
    GSPC.Open     = col_double(),
    GSPC.High     = col_double(),
    GSPC.Low      = col_double(),
    GSPC.Close    = col_double(),
    GSPC.Volume   = col_double(),
    GSPC.Adjusted = col_double()
  )
) %>%
  rename(DATE = Index) %>%
  arrange(DATE)

df_both <- left_join(sp500, df, by = "DATE") %>%
  filter(!is.na(GSPC.Close), !is.na(yield_curve))

min_max_scale <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

df_both <- df_both %>%
  mutate(
    log_sp500 = log(GSPC.Close)
  )

model_lm <- lm(log_sp500 ~ as.numeric(DATE), data = df_both)

df_both <- df_both %>%
  mutate(
    log_sp500_pred = predict(model_lm, newdata = df_both)
  )

df_both <- df_both %>%
  mutate(
    ratio_sp500       = log_sp500 / log_sp500_pred,
    scaled_ratio_sp500 = min_max_scale(ratio_sp500)
  )

df_both <- df_both %>%
  mutate(
    scaled_yield = min_max_scale(yield_curve)
  )

rng <- range(df_both$yield_curve, na.rm = TRUE)
min_val <- rng[1]
max_val <- rng[2]

scaled_zero <- (0 - min_val) / (max_val - min_val)

ggplot(df_both, aes(x = DATE)) +
  geom_line(aes(y = scaled_ratio_sp500, color = "Scaled: ln(S&P500) / ln(S&P500)_pred"),
            size = 1) +
  
  geom_line(aes(y = scaled_yield, color = "10Y-2Y (scaled)"), size = 1) +
  geom_hline(yintercept = scaled_zero, color = "blue", linetype="dotted") +
  geom_vline(
    data = filter(df, is_signal),
    aes(xintercept = DATE),
    color = "blue", 
    linetype = "dashed", 
    size = 0.8
  ) +
  labs(
    title = "Zależność krzywej rentowności do indeksu 500 największych spółek USA",
    x = "Data",
    y = "Wartość w skali (0, 1)"
  ) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(
    name = "",
    values = c("Scaled: ln(S&P500) / ln(S&P500)_pred" = "red",
               "10Y-2Y (scaled)" = "steelblue")
  ) +
  annotate("text",
           x = as.Date("2015-01-01"),
           y = scaled_zero,
           label = "10Y-2Y = 0",
           color = "blue",
           vjust = -1) +
  theme_minimal() +
  theme(legend.position = "top")

```

Wizualizacja pokazuje, że nie które sygnały nie zostały odfiltrowane tak jak założono, jednak by nie zawężać jego definicji, pozostawimy go takim jakim jest. Dokładnie przyglądając się, można ulec wrażeniu, że po sygnale, cena spada, ale przekształcimy ten wykres, aby pokazywał równolegle jak indeks zachowuje się po sygnale w skali ceny z momentu wystąpienia sygnału.

#### Postać ścieżkowa

```{r}
#| echo: false
library(purrr)

df_signals <- df_both %>%
  filter(is_signal) %>%
  select(DATE, GSPC.Close)

size <- 500

df_paths <- map_dfr(1:nrow(df_signals), function(i) {
  
  signal_day   <- df_signals$DATE[i]
  price_signal <- df_signals$GSPC.Close[i]
  
  start_row <- which(df_both$DATE == signal_day)
  end_row <- min(start_row + size - 1, nrow(df_both))
  
  df_sub <- df_both %>%
    slice(start_row:end_row)
  
  df_sub <- df_sub %>%
    mutate(
      rel_close          = GSPC.Close / price_signal,
      candles_from_signal = row_number() - 1,
      signal_date        = signal_day
    )
  
  df_sub
})

ggplot(df_paths, aes(
  x = candles_from_signal, 
  y = rel_close, 
  group = signal_date,
  color = factor(signal_date)
)) +
  geom_line(alpha = 0.7) +
  labs(
    title = "S&P 500 – względna zmiana w 2 latach po sygnale 10Y-2Y",
    subtitle = "Poszczególne sygnały, każda startuje z 1",
    x = "Świeczki od sygnału",
    y = "Relatywna cena (1 = cena w dniu sygnału)"
  ) +
  theme_minimal()


df_avg <- df_paths %>%
  group_by(candles_from_signal) %>%
  summarize(mean_rel_close = mean(rel_close, na.rm = TRUE)) %>%
  ungroup()

ggplot() +
  geom_line(
    data = df_paths,
    aes(x = candles_from_signal, y = rel_close, group = signal_date),
    color = "black",
    alpha = 0.3
  ) +
  geom_line(
    data = df_avg,
    aes(x = candles_from_signal, y = mean_rel_close),
    color = "red",
    size = 1
  ) +
  labs(
    title = "S&P 500 – względna zmiana w 2 latach po sygnale 10Y-2Y",
    subtitle = "Cienkie linie: poszczególne sygnały, gruba linia: średnia ścieżka",
    x = "Świeczki od sygnału",
    y = "Relatywna cena (1 = cena w dniu sygnału)"
  ) +
  theme_minimal()

```

Nie widać tu żadnych, szczególnych powtarzalności. Niektóre wykresy idą w góre, a inne w dół. Można zauważyć, że przez dwa lata, czyli około 500 dni roboczch na giełdzie, w pewnym momencie zachodzi istotny spadek ceny względem ceny z dnia sygnału. Tutaj spadki o co najmniej 20%.

```{r}
#| echo: false
df_paths_crossed <- df_paths %>%
  group_by(signal_date) %>%
  filter(signal_date == '1998-05-27') %>%
  ungroup()

df_paths_crossed <- df_paths %>%
  group_by(signal_date) %>%
  filter(any(rel_close < 0.80)) %>%
  ungroup()

ggplot(df_paths_crossed, aes(
  x = candles_from_signal, 
  y = rel_close, 
  group = signal_date,
  color = factor(signal_date)
)) +
  geom_line(alpha = 0.7) +
  geom_hline(yintercept = 0.8, color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "S&P 500 – tylko ścieżki przecinające 0.8",
    subtitle = "Każda linia startuje z 1, a w pewnym momencie spada < 0.8",
    x = "Świeczki od sygnału",
    y = "Relatywna cena (1 = cena w dniu sygnału)"
  ) +
  theme_minimal()
```

#### Podsumowanie ścieżek i klasteryzacja

Przekształcając dane w podsumowania:

```{r}
#| echo: false
library(dplyr)
library(tidyr)
library(ggplot2)

calc_increases_decreases <- function(x) {
  lowest_so_far <- x[1]
  highest_increase <- 0
  for (val in x) {
    if (val < lowest_so_far) {
      lowest_so_far <- val
    }
    current_increase <- val - lowest_so_far
    if (current_increase > highest_increase) {
      highest_increase <- current_increase
    }
  }
  
  highest_so_far <- x[1]
  highest_decrease <- 0
  for (val in x) {
    if (val > highest_so_far) {
      highest_so_far <- val
    }
    current_decrease <- highest_so_far - val
    if (current_decrease > highest_decrease) {
      highest_decrease <- current_decrease
    }
  }
  
  list(
    highest_increase = highest_increase,
    highest_decrease = highest_decrease
  )
}

df_paths <- df_paths %>%
  group_by(signal_date) %>%
  arrange(candles_from_signal) %>%
  mutate(daily_ret = rel_close / lag(rel_close) - 1) %>%
  ungroup()

df_summary <- df_paths %>%
  group_by(signal_date) %>%
  summarize(
    min_rel_close  = min(rel_close, na.rm = TRUE),
    max_rel_close  = max(rel_close, na.rm = TRUE),
    mean_rel_close = mean(rel_close, na.rm = TRUE),
    sd_daily_ret   = sd(daily_ret, na.rm = TRUE),
    
    stats = list(calc_increases_decreases(rel_close))
  ) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    highest_increase = stats$highest_increase,
    highest_decrease = stats$highest_decrease
  ) %>%
  ungroup() %>%
  select(-stats)

df_summary_long <- df_summary %>%
  pivot_longer(
    cols = -signal_date,
    names_to = "metric",
    values_to = "value"
  ) %>%
  mutate(
    label_value = sprintf("%.2f%%", value * 100)
  )


library(reactable)
library(reactablefmtr)

reactable(
  df_summary,
  # Domyślnie wyświetlaj do 3 miejsc po przecinku
  defaultColDef = colDef(
    format = colFormat(digits = 3)
  ),
  columns = list(
    signal_date = colDef(
      name = "Data sygnału"
    ),
    min_rel_close = colDef(
      name = "Minimum (rel. Close)"
    ),
    max_rel_close = colDef(
      name = "Maksimum (rel. Close)"
    ),
    mean_rel_close = colDef(
      name = "Średnia (rel. Close)"
    ),
    sd_daily_ret = colDef(
      name = "Odchylenie std (dzienne)",
      format = colFormat(digits = 5)  # 5 miejsc po przecinku
    ),
    highest_increase = colDef(
      name = "Największy wzrost",
      cell = data_bars(
        df_summary, 
        fill_color = "lightgreen", 
        text_position = "outside-end"
      )
    ),
    highest_decrease = colDef(
      name = "Największy spadek",
      cell = data_bars(
        df_summary, 
        fill_color = "lightcoral",
        text_position = "outside-end"
      )
    )
  ),
  bordered = TRUE,
  striped = TRUE,
  highlight = TRUE,
  showPageSizeOptions = TRUE,
  pageSizeOptions = c(5, 10, 15)
)



```

otrzymamy postać możliwą do sklastrowania, co zostanie wykonane, ale przed tym zostaną dodane sygnały losowe tj. losowo wybrane przedziały pięćset świeczkowe rozdzielne z przedziałami ścieżek sygnałów prawdziwych.

```{r}
#| echo: false
library(dplyr)
library(purrr)
library(ggplot2)
library(factoextra) 

n_total <- nrow(df_both)
max_start <- n_total - 499
possible_starts <- 1:max_start

real_signal_rows <- match(df_signals$DATE, df_both$DATE)

bad_indices <- c()
for (s in real_signal_rows) {
  lower <- max(1, s - 499)
  upper <- min(max_start, s + 499)
  bad_indices <- c(bad_indices, lower:upper)
}
bad_indices <- unique(bad_indices)

possible_starts_clean <- setdiff(possible_starts, bad_indices)

set.seed(2025)
random_starts <- sample(possible_starts_clean, size = 50, replace = FALSE)

df_random_signals <- data.frame(
  random_signal_date = df_both$DATE[random_starts],
  start_row = random_starts
)

df_paths_random <- map_dfr(1:nrow(df_random_signals), function(i) {
  
  start_row <- df_random_signals$start_row[i]
  end_row   <- start_row + 499
  
  df_sub <- df_both %>%
    slice(start_row:end_row)
  
  price_signal <- df_sub$GSPC.Close[1]
  
  df_sub <- df_sub %>%
    mutate(
      random_signal_date = df_random_signals$random_signal_date[i],
      candles_from_signal = row_number() - 1,  # 0..499
      rel_close = GSPC.Close / price_signal
    )
  
  df_sub
})

df_paths_random <- df_paths_random %>%
  group_by(random_signal_date) %>%
  arrange(candles_from_signal) %>%
  mutate(
    daily_ret = rel_close / lag(rel_close) - 1
  ) %>%
  ungroup()

df_summary_random <- df_paths_random %>%
  group_by(random_signal_date) %>%
  summarize(
    min_rel_close  = min(rel_close, na.rm = TRUE),
    max_rel_close  = max(rel_close, na.rm = TRUE),
    mean_rel_close = mean(rel_close, na.rm = TRUE),
    sd_daily_ret   = sd(daily_ret, na.rm = TRUE),
    stats = list(calc_increases_decreases(rel_close))
  ) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    highest_increase = stats$highest_increase,
    highest_decrease = stats$highest_decrease
  ) %>%
  ungroup() %>%
  select(-stats) %>%
  mutate(is_signal = FALSE)

df_summary_real <- df_summary %>%
  rename(signal_date = signal_date) %>%
  mutate(is_signal = TRUE)

# Łączymy
df_summary_combined <- df_summary_real %>%
  select(signal_date, min_rel_close, max_rel_close, mean_rel_close,
         sd_daily_ret, highest_increase, highest_decrease, is_signal) %>%
  bind_rows(
    df_summary_random %>%
      rename(signal_date = random_signal_date) %>%
      select(signal_date, min_rel_close, max_rel_close, mean_rel_close,
             sd_daily_ret, highest_increase, highest_decrease, is_signal)
  )

df_cluster_data <- df_summary_combined %>%
  select(min_rel_close, max_rel_close, mean_rel_close,
         sd_daily_ret, highest_increase, highest_decrease)

df_cluster_scaled <- scale(df_cluster_data)

set.seed(123)
k <- 3
kmeans_res <- kmeans(df_cluster_scaled, centers = k, nstart = 100)

df_summary_combined$cluster <- factor(kmeans_res$cluster)

fviz_cluster(
  object = kmeans_res,
  data = df_cluster_scaled,
  geom = "point",
  ellipse.type = "convex",
  palette = "jco",
  main = "K-means (k=3) na metrykach ścieżek"
)

pca_res <- prcomp(df_cluster_scaled, scale. = FALSE)
pca_df <- as.data.frame(pca_res$x[, 1:2])
pca_df$cluster <- df_summary_combined$cluster
pca_df$is_signal <- df_summary_combined$is_signal

pca_df <- pca_df %>%
  mutate(is_signal = ifelse(is_signal, "REALNY", "LOSOWY"))

ggplot(pca_df, aes(
  x = PC1, y = PC2,
  color = cluster,
  shape = is_signal
)) +
  geom_point(size = 3) +
  labs(
    title = "Wizualizacja PCA – klasteryzacja sygnały realne/losowe",
    color = "Klaster",
    shape = "Rodzaj sygnału"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )
```

Nie wykazało to, żadnych istotnych różnic pomiędzy sygnałami losowymi, a realnymi. Prawdziwe sygnały są równo rozłożone po klastrach.

#### Podsumowanie

10Y-2Y zdefiniowane wyżej nie daje pożądanych rezultatów. Na wizualizacjach widać spadki po jego wystąpieniu, ale nigdy nie wiadomo kiedy i o ile, co czyni go nieefektywnym. Prawdopodobnym jest, że sygnał powinien zostać zdefiniowany, nie tylko warunkami historycznymi, ale też przyszłymi, czyli wychodzącymi z pozycji w przypadku nie porządanych ruchów rynku np. odwróceniu, a sygnał powinien posiadać swoje metadane takie jak długość odwrócenia krzywej rentowności, pole wykresu pod 10Y-2Y=0 czy wysokość trendu wzrostowego (np. przez SMA).

### Rodzina sygnałów SMA (Simple Moving Average)

#### Definicja

Właśnie do tego przechodzimy. SMA-i w czasie $t$, to średnia arytmetyczna z $i$ ostatnich cen.

$$
SMA_i(t) = \frac{cena_t + cena_{t-1} + \dots + cena_{t-i+1}}{i} = \frac{1}{i} \sum_{k=t-i+1}^{t} a_k
$$

Dodatkowo mamy pięć paramterów:

-   **entry_short** - cena wejścia w pozycję short
-   **entry_long** - cena wejścia w pozycję long
-   **exit_short_threshold** - cena wyjścia z pozycji short
-   **exit_long_threshold** - cena wyjścia z pozycji long
-   **position_timeout** - czas po którym pozycja się zamyka

Wartości cenowe nie są wyrażanie w walutach, a w relacji do SMA czyli $\text{wartość\_parametru} = \frac{cena_{teraz}}{SMA_i(t)}$. Czas zamknięcia pozycji ustawiona będzie na stałe 14 dni.

Otwieranie pozycji odbywa się, gdy $\text{wartość\_parametru} > \text{entry\_short}$ dla krótkiej pozycji, oraz $\text{wartość\_parametru} < \text{entry\_long}$ dla długiej.

#### Algorytm kombinacji parametrów

Analiza SMA o wyznaczonych paramterach jest mało efektywna. Ilość potencjalnie efektywnych parametrów jest za duża, aby szukać ich ręcznie. Dobrym rozwiązaniem jest użycie algorytmu, który testuje kombinacje parametrów, co pewien skok, tworząc bazę danych wszystkich "kombinacji". Posłużymy się taką bazą, wygenerowaną w oparciu o dane historyczne pary BTCUSD o długości 10 lat od września 2014 do sierpnia 2024. W bazie są dodatkowe parametry:

-   **i** - parametryzujemy okno SMA
-   **multiple_entries** - raczej pominiemy ze względu na abstrakcyjność działania (zakłada możliwość wchodzenia w pozycje wielokrotnie)
-   **capital_percent** - ilość kapitału wchodząca w pozycje

oraz zmienne wynikowe:

-   **total_short_return** - suma zwrotów z pozycji short (mało przydatne)
-   **total_long_return** - suma zwrotów z pozycji long (mało przydatne)
-   **total_return** - suma dwóch poprzednich
-   **positions** - ilość pozycji przydatna do oceny skali zabrania prowizji przez brokera oraz spreadu
-   **balance** - saldo końcowe (na początku równe 1)
-   **ROI** - roczna stopa zwrotu

Przejdziemy do analizy wyników z bazy danych o ponad 130 tysiącach różnych kombinacji.

##### Zakres parametrów

Poniżej przedstawiono zakresy użytych parametrów algorytmu:

$$ i \in \{3, 4, 5, 6\} $$

$$ \text{entry\_short} \in [1.00, 1.05], \text{ krok: } 0.01 $$

$$ \text{entry\_long} \in [0.95, 0.99], \text{ krok: } 0.01 $$

$$ \text{exit\_short\_threshold} \in [0.95, 0.99], \text{ krok: } 0.01 $$

$$ \text{exit\_long\_threshold} \in [\text{entry\_long}, 1.05], \text{ krok: } 0.01 $$

$$ \text{capital\_percent} \in [0.1, 0.9], \text{ krok: } 0.1 $$

$$ \text{position\_timeout} = 14 \text{ dni} $$

Ze względu na moce obliczeniowe, parametry zostały tak ograniczone, jednak ciekawe byłoby ustawienie entry_short też na wartości mniejsze od SMA, co znaczyło by przewidywanie dalszego trendu spadkowego, oraz entry_long na wartości na wartości większe od 1, czyli kontynuowanie trendu wzrostowego.

#### Rozkład wydajności sygnałów

Domyślnie używamy danych o paramaterze: $\text{multiple\_entries} = FALSE$

```{r}
#| echo: false

library(ggplot2)
library(dplyr)
library(plotly)

# Liczone za pomocą SMA_algorithm.py na danych z pary BTCUSD w okresie od 2014-09-17 do 2024-08-15, czyli 9.9110198494 lat
data <- read.csv("SMA_algorithm_performance_BTCUSD_1d_ALL.csv", stringsAsFactors = FALSE)

data <- data %>%
  mutate(
    i = as.numeric(i),
    entry_short = as.numeric(entry_short),
    entry_long = as.numeric(entry_long),
    exit_short_threshold = as.numeric(exit_short_threshold),
    exit_long_threshold = as.numeric(exit_long_threshold),
    position_timeout = as.character(position_timeout),
    multiple_entries = as.logical(multiple_entries),
    capital_percent = as.numeric(capital_percent),
    total_short_return = as.numeric(total_short_return),
    total_long_return = as.numeric(total_long_return),
    total_return = as.numeric(total_return),
    positions = as.integer(positions),
    balance = ifelse(balance <= 0, 0, as.numeric(balance)),
    roi = ifelse(balance > 0, ((balance^(1 / 9.9110198494)) - 1) * 100, -100.0)   
  )

# Surowa postać zawiera roi = 0 w przypadku balance = 0, ponieważ zakłada likwidacje przy ujemnym saldzie. Nie byłem pewny na jakim przedziale odbywała się egzekucja algorytmu, ale z roi dodatnich oraz balansu dało się wyliczyć dokładną długość danych historycznych. czyli ze wzoru t = ln(balance)/(1 + roi). Potwierdziło się to z danymi historycznymi które znajdowały się w tym samym folderze.
# Mówiąc o roi mam na myśli roczną stopę zwrotu.


data <- data %>%
  filter(multiple_entries == FALSE)

top_n_data <- data %>%
  group_by(i) %>%
  arrange(desc(roi)) %>%
  slice(1:10) %>%
  mutate(rank = row_number()) %>%
  ungroup()

heatmap <- ggplot(top_n_data, aes(x = factor(i), y = rank, fill = roi, text = paste(
  "i:", i,
  "<br>Ranking:", rank,
  "<br>ROI:", roi,
  "<br>Wejście Short:", entry_short,
  "<br>Wejście Long:", entry_long,
  "<br>Wyjście Short:", exit_short_threshold,
  "<br>Wyjście Long:", exit_long_threshold,
  "<br>Zwrot Long:", total_long_return,
  "<br>Zwrot Short:", total_short_return,
  "<br>Procent kapitału:", capital_percent,
  "<br>Saldo:", balance,
  "<br>Pozycje:", positions
))) +
  geom_tile() +
  scale_fill_gradientn(
    colors = c("blue", "cyan", "green", "yellow", "orange", "red"),
    breaks = seq(min(top_n_data$roi, na.rm = TRUE), max(top_n_data$roi, na.rm = TRUE), length.out = 6),
    name = "ROI (%)"
  ) +
  scale_y_reverse(breaks = 1:10) +
  labs(
    title = "Ranking 10 największych ROI po rozmiarze okna SMA",
    x = "SMA (i)",
    y = "Ranking"
  ) +
  theme_minimal()

interactive_heatmap <- ggplotly(heatmap, tooltip = "text")

interactive_heatmap
```


Jak widać, najlepiej radzi sobie algorytm używający $SMA_3$. Wydajność maleje wraz ze wzrostem okna $i$, co jest ciekawą informacją z niepewną interpretacją. Zwracając uwagę na najbardziej efektywny algorytm można spojrzeć, źe parametry są dosyć wyjątkowe. Wejście w pozycje long odbywa się, gdy cena jest taka jak średnia, lub niższa. Sygnałów long jest bardzo dużo. Otwarcia pozycji short są rzadkie, ponieważ cena musi być wyższa o 5% w stosunku do trzy dniowej średniej, w której jest wliczona obecna cena. Tu warto wspomnieć, że interpretacja wzrostu o 5% w ciągu ostatniego dnia, jest błędna na ogół. Zakładając dla $SMA_3$

$$a_n = x,\ a_{n-1} = 1000,\ a_{n-2} = 1000$$
oraz
$$\text{wartość\_parametru} = 1.05$$
okazuje się, że
$$x \approx 1077$$
co przekłada się na prawie **8%** dzienny wzrost. Spowodowane jest to oczywiście wpływem $a_n$ na wartość $SMA$. Taki spadek, lub dwa dni spadkowe pod rząd, występują rzadziej co przekłada się na mniej sygnałów. Wydajność akurat przy tych parametrach pozycji short jest to dosyć zrozumiała, kiedy spojrzymy na to iż nie ma ani jednej kombinacji parametrów, w której **total_short_return** jest dodatni, co jest z resztą zadziwiajace, że przy tylu iteracjach nie znalazła się nawet jedna. Wszystko to się zgadza również z faktem, że bitcoin wykazuje lepsze tempo wzrostu od SP&500, a tego logarytmując w poprzednim sygnale, ukazujemy jako funkcję wykładniczą. To jednak nie usprawiedliwia algorytmu, i należy stwierdzić, że oferuje on słabe sygnały krótkie.


#### Najmniej wydajne sygnały

Pokazanie najmniej wydajnych sygnałów jest możliwe jedynie przez ujawnienie całego rozkładu, ponieważ jest na poziomie doprowadzającym saldo do 0 w dużej ilości przypadków.

```{r}
#| echo: false

top_n_data <- data %>%
  group_by(i) %>%
  arrange(desc(roi)) %>%
  mutate(rank = row_number()) %>%
  ungroup()

heatmap <- ggplot(top_n_data, aes(x = factor(i), y = rank, fill = roi, text = paste(
  "i:", i,
  "<br>Ranking:", rank,
  "<br>ROI:", roi,
  "<br>Wejście Short:", entry_short,
  "<br>Wejście Long:", entry_long,
  "<br>Wyjście Short:", exit_short_threshold,
  "<br>Wyjście Long:", exit_long_threshold,
  "<br>Procent kapitału:", capital_percent,
  "<br>Saldo:", balance,
  "<br>Pozycje:", positions
))) +
  geom_tile() +
  scale_fill_gradientn(
    colors = c("blue", "cyan", "green", "yellow", "orange", "red"),
    breaks = seq(min(top_n_data$roi, na.rm = TRUE), max(top_n_data$roi, na.rm = TRUE), length.out = 6),
    name = "ROI (%)"
  ) +
  scale_y_reverse(breaks = 1:10) +
  labs(
    title = "Ranking największych ROI po rozmiarze okna SMA",
    x = "SMA (i)",
    y = "Ranking"
  ) +
  theme_minimal()

interactive_heatmap <- ggplotly(heatmap, tooltip = "text")

interactive_heatmap

```

Powodem dla którego na rozkładzie widać granice oddzielenia niebieskiego pola, jest sposób wykonywania operacji zmiennoprzecinkowych. Pomimo poprawnych matematycznie obliczeń, saldo potrafi wyjść ujemne, bardzo bliskie zeru, co uniemożliwa obliczenia ROI za pomocą pierwiastkowania. Wtedy saldo przyjmujemy jako 0, a ROI jako -100%.

```{r}
#| echo: false

library(dplyr)
library(ggplot2)
library(plotly)

top_n_data <- data %>%
  group_by(i) %>%
  arrange(desc(roi)) %>%
  mutate(rank = row_number()) %>%
  ungroup()

ggplot(data, aes(x = roi, y = ..count.., fill = ..x..)) +
  geom_histogram(bins = 50, alpha = 0.7) +
  scale_fill_gradientn(
    colors = c("blue", "cyan", "green", "yellow", "orange", "red"),
    breaks = seq(min(data$roi, na.rm = TRUE), max(data$roi, na.rm = TRUE), length.out = 6),
    name = "ROI (%)"
  ) +
  labs(
    title = "Rozkład ROI",
    x = "ROI (%)",
    y = "Częstotliwość"
  ) +
  theme_minimal()
```

Mało konkretnych wniosków da się wysunąć z tych wykresów, chociaż dają nam spojrzenie na ogólną wydajność różnych $i$ za pomocą heatmapy, oraz na wydajność ogólną algorytmu za pomocą rozkładu ROI. Jest ona dosyć kiepska

#### Strategie pozycji

```{r}
#| echo: false

ggplot(data, aes(x = capital_percent, y = roi)) +
  geom_point(alpha = 0.5, color = "purple") +
  labs(title = "ROI vs. Procent kapitału",
       x = "Procent kapitału",
       y = "ROI (%)") +
  theme_minimal()


```

Na początek spójrzmy na wydajność algorytmu, względem procentu kapitału w pozycji. Nie ma tu zaskoczenia. Czym więcej środków, tym większy poziom ryzyka. W przypadku dobrych parametrów sygnału, opłaca się otwierać większe pozycje. Przyda nam się pamiętać to przy następnych wykresach.

```{r}
#| echo: false
plot_ly(data, x = ~entry_short, y = ~exit_short_threshold, z = ~total_short_return,
        type = 'scatter3d', mode = 'markers',
        marker = list(size = 5, color = ~roi, colorscale = 'Viridis')) %>%
  layout(
    scene = list(
      xaxis = list(title = "Wejście w pozycje short"),
      yaxis = list(title = "Wyjście z pozycji short"),
      zaxis = list(title = "Suma rentowności pozycji short (total_short_return)"),
      camera = list(
        eye = list(x = -2.0, y = -1.3, z = 0.3)
      )
    ),
    title = "Wykres zależności strategi pozycji short, do jej wyników"
  )

plot_ly(data, x = ~entry_long, y = ~exit_long_threshold, z = ~total_long_return,
        type = 'scatter3d', mode = 'markers',
        marker = list(size = 5, color = ~roi, colorscale = 'Viridis')) %>%
  layout(
    scene = list(
      xaxis = list(title = "Wejście w pozycje long"),
      yaxis = list(title = "Wyjście z pozycji long"),
      zaxis = list(title = "Suma rentowności pozycji long (total_long_return)"),
      camera = list(
        eye = list(x = -2.0, y = -1.3, z = 0.3)
      )
    ),
    title = "Wykres zależności strategi pozycji long, do jej wyników"
  )

```

Sygnał pozycji short, oraz pozycji long znacząco różnią się od siebie. Pierwsze co się wyróżnia, to że skala ma zupełnie inne wartości. Shorty - tylko ujemne, a longi tylko dodatnie. Dodatkowo wykres pozycji długich jest wyższy, oraz ma tendendę wzrostową przy wyjściu z pozycji równym $1.05$. Wykres pozycji krótkich jest jak lekko pochylona płaszczyzna. Na obu wykresach, dla jednej kombinacji parametrów pozycji short znajduje się kilka punktów, ponieważ są to wartości dla różnych *capital_percent*.

#### Podsumowanie

Wyniki są bardzo ciekawe, chociaż sygnały nie dają dużego zwrotu w rzeczywistości, pomimo że pokazuje to najwydajniejsza pozycja (38% roczny wzrost). Zyski zjada prowizja brokera i spread ceny. Kolejnym aspektem jest, że algorytm po prostu zawsze kupował poniżej średniej i trzymał jak najdłużej. Cały trud mógł zrobić wykładniczy wzrost ceny bitcoina. Kierowanie się pojedyńczym SMA jak widać nie jest efeketywnym sygnałem rynkowym. Kolejnymi krokami rozwijania koncepcji opartej o SMA, może być zwiększenie ilości iterowanych parametrów, zmiana długości zbioru danych na dłuższy, podział takiego zbioru na części i sprawdzanie poszczególnych wydajności (np. w bull market lub bear market), oraz zwiększenie ilości SMA, do np. kilku co spotęgowało by skomplikowanie algorytmu.

## Mowa końcowa

Najprostsze sygnały rynkowe nie są pomocne do oceny stanu rynku. Giełda to gra konkurencjyna, w której znalezienie dobrego prostego sygnału, spotka się od razu z otwarciem pozycji przez graczy, i w konsekwencji zmiany ceny, co czyni sygnał niedziałającym. Omawiane sygnały wymagają dalszej, o wiele głębszej analizy, aby dojść do sygnałów rentownych. Jednak wierzę, że tak prosta i podstawowa analiza jest w stanie otworzyć umysły na nieszablonowe myślenie oraz tworzyć pomysły na trudniejsze analizy. Uchroni także przed wierzeniem w pozornie działające sygnały pokazywane w internecie, gdzie autorzy zapewniają o dużych zyskach w krótkim czasie. Ważnym jest aby nie tylko zarobić, ale też nie stracić.